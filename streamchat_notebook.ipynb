{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "streamchat_attempt2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm9ndYlrJVzB"
      },
      "source": [
        "# GTP-2 Language Model fine-tuning from scratch\n",
        "An attempt at fine-tuning GTP-2 to generate livestream messages based on compiled data from 10 x 6-8hour livestreams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLf7cATJRJu"
      },
      "source": [
        "# INSTALL DEPENDENCIES\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# Expect: transformers --3.3.0 / tokenizers --0.8.1rc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS4mM-pqKNeq"
      },
      "source": [
        "Need to train a byte-level BPE tokenizer for GTP-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax3QRsjhKBm0"
      },
      "source": [
        "%%time \n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"compiled.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=100_000, min_frequency=2, special_tokens=[\n",
        "    \"<BOS>\",\n",
        "    \"<EOS>\",\n",
        "    \"<PAD>\",\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy6Q5-LcKpYl"
      },
      "source": [
        "Save the model to directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-hsjknJKbyZ"
      },
      "source": [
        "!mkdir streamchat_model\n",
        "tokenizer.save_model(\"streamchat_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsBvtN9MK9HA"
      },
      "source": [
        "Test the tokenizer out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLzy4s-wLAua"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./streamchat_model/vocab.json\",\n",
        "    \"./streamchat_model/merges.txt\",\n",
        ")\n",
        "\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"<EOS>\", tokenizer.token_to_id(\"<EOS>\")),\n",
        "    (\"<BOS>\", tokenizer.token_to_id(\"<BOS>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "tokenizer.encode(\"Those are some Pog flowers.\").tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Mprm7WLVa9"
      },
      "source": [
        "# Training the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxDgrS-WLOcD"
      },
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD0x1OSULe0k"
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXpjAa5iLiYZ"
      },
      "source": [
        "Define the config for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqw0pGRwLm8P"
      },
      "source": [
        "from transformers import GPT2Config\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=100_000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-jx5Zo5MOXN"
      },
      "source": [
        "Re-create the tokenizer but with transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV4oCQ60MF3k"
      },
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"./streamchat_model\", bos_token=\"<BOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdbO1tvYMqR_"
      },
      "source": [
        "Initialise the model with the config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tLgNolCMado"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel(config=config)\n",
        "model.num_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myU0AfP-Mx-N"
      },
      "source": [
        "# Build the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyOEj2pqM3kj"
      },
      "source": [
        "Build the dataset by applying the custom tokenizer to text file\n",
        "As using a single textfile as data source, just use `LineByLineDataset` out-of-the-box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EPEK1RwMzoK"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "#LineByLine requires a pad_token in the tokenizer\n",
        "train_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./train.txt\", #PUT THE DATASET IN HERE\n",
        "    block_size=180,\n",
        ")\n",
        "eval_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./test.txt\", #PUT THE DATASET IN HERE\n",
        "    block_size=180,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqSM0UbNNKJi"
      },
      "source": [
        "Need to define a data_collator so that we can batch different samples of the input data into a PyTorch compatible object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SQh_dmNZm2"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "# MLM is masked language modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmbDaFM5OM1U"
      },
      "source": [
        "Initialise the trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRaoeZPwOOsi"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./streamchat_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=20,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=0,\n",
        "    prediction_loss_only=True,\n",
        "    do_train=True,\n",
        "    do_eval=True, \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhgsKA5cOi4X"
      },
      "source": [
        "Begin training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4rPtqRtOkno"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLEtXuZQOpUa"
      },
      "source": [
        "Save the final model (+ tokenizer + config)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2mopFtdOtWr"
      },
      "source": [
        "trainer.save_model(\"./streamchat_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU-5ftMcOzZ7"
      },
      "source": [
        "Apparently it's over..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvxDL-E_1LBH"
      },
      "source": [
        "!zip -r /content/streamchat_model.zip /content/streamchat_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0AfP9jHDm3E"
      },
      "source": [
        "# Text generation\n",
        "\n",
        "Try using the model to run some text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4rTjmkZDsPY"
      },
      "source": [
        "!python run_generation.py --model_type=\"gpt2\" --model_name_or_path=\"./streamchat_model\" --prompt=\"<BOS>\" --stop_token=\"<EOS>\" --length=200 --k=30 --num_return_sequences=40"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}